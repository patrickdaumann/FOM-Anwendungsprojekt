{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Übersicht Algorithmen\n",
    "1. Naive Bayes-Klassifikation\n",
    "2. Lineare Regression\n",
    "3. Support Vector Machine\n",
    "4. Entscheidungsbaum / Random Forest\n",
    "5. Hauptkomponentenanalyse\n",
    "6. Manifold Learning\n",
    "7. K-Means Clustering\n",
    "8. Gauß’sche Mixture-Modelle\n",
    "9. Kerndichteschätzung\n",
    "\n",
    "Generell wichtige Anforderungen:\n",
    "-Qualität der Daten: Die Daten sollten von guter Qualität sein, d.h., sie sollten frei von zu vielen fehlenden Werten, Ausreißern oder fehlerhaften Einträgen sein.\n",
    "-Verständnis der Daten: Ein grundlegendes Verständnis der Daten und ihres Kontextes ist wichtig, um sicherzustellen, dass die Ergebnisse korrekt interpretiert werden.\n",
    "-Skalierung der Merkmale: Es ist hilfreich, die Merkmale zu skalieren, insbesondere wenn sie unterschiedliche Maßeinheiten oder Größenordnungen haben.\n",
    "-Fehlende Daten: Modelle können empfindlich auf fehlende Daten reagieren. Es ist wichtig, fehlende Daten angemessen zu behandeln, sei es durch Imputation oder durch Entfernen von Datensätzen mit fehlenden Werten.\n",
    "-Ausreißer: Manche Modelle können empfindlich auf Ausreißer reagieren, da diese die Schätzung der Mittelwerte und Kovarianzen der Komponenten beeinflussen können. Es kann notwendig sein, Ausreißer zu identifizieren und zu behandeln."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes-Klassifikation - Klassifizierung\n",
    "Das Bayessche Klassifikationsverfahren, oft auch als Naive-Bayes-Klassifikator bezeichnet, ist ein statistischer Ansatz zur Klassifizierung, um Vorhersagen über die Zugehörigkeit von Objekten zu bestimmten Klassen zu treffen.\n",
    "Dabei handelt es sich um eine Gleichung, die die Beziehung zwischen bedingten\n",
    "Wahrscheinlichkeiten statistischer Größen beschreibt.\n",
    "Bei der Bayes-Klassifikation möchten wir anhand der beobachteten Merkmale die\n",
    "Wahrscheinlichkeit eines Labels ermitteln.\n",
    "\n",
    "Die Annahme einer Gaußverteilung der Daten ohne Kovarianz zwischen den Dimensionen ist\n",
    "eine besonders schnelle Möglichkeit, ein einfaches Modell zu erstellen.\n",
    "\n",
    "Anforderung:\n",
    "Wir können dieses Modell anpassen, indem wir einfach den Mittelwert und die\n",
    "Standardabweichung der Punkte eines Labels berechnen – mehr ist für die Definition eines\n",
    "solchen Modells nicht erforderlich.\n",
    "-Merkmalsunabhängigkeit\n",
    "-Verteilungsannahmen: Für bestimmte Varianten des Naive-Bayes, wie den Gaußschen Naive-Bayes, ist es wichtig, dass die Verteilung der kontinuierlichen Merkmale in jeder Klasse ungefähr normal (gaußförmig) ist.\n",
    "\n",
    "\n",
    "Beispiel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1]:\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "In[2]:\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(100, 2, centers=2, random_state=2, cluster_std=1.5)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lineare Regression - Zusammenhang zwischen einer abhängigen Variablen und einer oder mehreren unabhängigen Variablen zu modellieren. \n",
    "Verlauf der Daten veranschaulichen\n",
    "Bei der einfachen linearen Regression wird eine Beziehung zwischen genau zwei Variablen untersucht: einer unabhängigen Variablen (x) und einer abhängigen Variablen (y). Die Beziehung wird durch eine gerade Linie dargestellt, die durch die Datenpunkte gelegt wird, um den Zusammenhang zwischen x und y zu modellieren.\n",
    "\n",
    "ein lineares Regressionsmodell ein guter Ausgangspunkt für Regressionsaufgaben. Diese\n",
    "Modelle sind so beliebt, weil sie sich sehr schnell anpassen lassen und auch sehr gut verständlich sind.\n",
    "Die einfachste Form einer linearen Regression (die Anpassung einer Geraden an die Daten) dürfte\n",
    "Ihnen geläufig sein, solche Modelle können aber auch erweitert werden, um einem komplizierteren\n",
    "Verhalten der Daten Rechnung zu tragen.\n",
    "\n",
    "Geht auch mehrdimensional:\n",
    "Bei der multiplen linearen Regression wird die Beziehung zwischen einer abhängigen Variablen und zwei oder mehr unabhängigen Variablen untersucht. Das Modell versucht, eine Hyperebene zu finden, die am besten zu den Datenpunkten passt.\n",
    "Der LinearRegression-Schätzer kann auch mit mehrdimensionalen linearen Modellen der Form\n",
    "y = a0 + a1x1 + a2x2 + ... umgehen, die mehrere x-Werte enthalten. Geometrisch entspricht das in drei\n",
    "Dimensionen dem Anpassen einer Ebene an räumliche Punkte oder in höheren Dimensionen dem\n",
    "Anpassen einer Hyperebene an mehrdimensionale Punkte.\n",
    "Die Multidimensionalität solcher Regressionen erschwert es, sie zu visualisieren.\n",
    "\n",
    "Das Nutzen linearer Regressionen mit Basisfunktionen macht die Modelle zwar flexibler, es kann jedoch\n",
    "auch sehr schnell zu einer Überanpassung führen.\n",
    "\n",
    "Trick:\n",
    "Um eine lineare Regression an nichtlineare Beziehungen zwischen den Variablen anzupassen, können\n",
    "Sie den Trick nutzen, die Daten entsprechend einer Basisfunktion zu transformieren. Beachten Sie, dass es sich noch immer um ein lineares Modell handelt – die Linearität bezieht sich\n",
    "darauf, dass die Koeffizienten an niemals miteinander multipliziert oder durcheinander geteilt werden.\n",
    "Faktisch haben wir die eindimensionalen x-Werte in eine höhere Dimension projiziert, sodass sich der\n",
    "lineare Fit an kompliziertere Beziehungen zwischen x und y anpassen kann.\n",
    "\n",
    "Anforderungen:\n",
    "Linearität: Die wichtigste Annahme ist, dass eine lineare Beziehung zwischen den unabhängigen Variablen und der abhängigen Variablen besteht. Das bedeutet, dass eine Änderung in einer unabhängigen Variablen zu einer proportionalen Änderung in der abhängigen Variablen führt.\n",
    "Unabhängigkeit der Beobachtungen: Die Beobachtungen sollten unabhängig voneinander sein. In Zeitreihendaten beispielsweise kann diese Annahme verletzt sein, da aufeinanderfolgende Beobachtungen oft korreliert sind.\n",
    "Homoskedastizität: Die Varianz der Residuen (die Unterschiede zwischen den beobachteten und den durch das Modell vorhergesagten Werten) sollte für alle Werte der unabhängigen Variablen konstant sein. Wenn die Varianz der Residuen mit den unabhängigen Variablen variiert, spricht man von Heteroskedastizität, was zu Problemen bei der Schätzung und Interpretation führen kann.\n",
    "Normalverteilung der Fehlerterme: Für viele statistische Tests und Intervallschätzungen ist es erforderlich, dass die Residuen normalverteilt sind. Dies ist besonders wichtig, wenn die Stichprobengröße klein ist.\n",
    "Keine oder geringe Multikollinearität: Wenn es starke Korrelationen zwischen zwei oder mehr unabhängigen Variablen gibt, kann dies zu Problemen bei der Bestimmung des Einflusses jeder unabhängigen Variablen auf die abhängige Variable führen.\n",
    "Ausreißer: Ausreißer können die Ergebnisse der linearen Regression stark beeinflussen, da sie die Schätzung der Regressionskoeffizienten verzerren können. Es ist wichtig, die Daten auf Ausreißer zu überprüfen und geeignete Maßnahmen zu ergreifen.\n",
    "Angemessene Stichprobengröße: Um zuverlässige Schätzungen zu erhalten, ist eine ausreichend große Stichprobe erforderlich. Die erforderliche Stichprobengröße hängt von der Anzahl der unabhängigen Variablen und der Komplexität des Modells ab.\n",
    "Messgenauigkeit: Die Variablen sollten genau und konsistent gemessen werden. Messfehler können zu verzerrten Schätzungen und falschen Schlussfolgerungen führen.\n",
    "Additivität: Die Effekte der unabhängigen Variablen auf die abhängige Variable sollten additiv sein. Das bedeutet, dass der Gesamteffekt auf die abhängige Variable die Summe der einzelnen Effekte ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[2]:\n",
    "rng = np.random.RandomState(1)\n",
    "x = 10 * rng.rand(50)\n",
    "y = 2 * x - 5 + rng.randn(50)\n",
    "plt.scatter(x, y);\n",
    "\n",
    "\n",
    "In[3]:\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "xfit = np.linspace(0, 10, 1000)\n",
    "yfit = model.predict(xfit[:, np.newaxis])\n",
    "plt.scatter(x, y)\n",
    "plt.plot(xfit, yfit);\n",
    "\n",
    "In[5]:\n",
    "rng = np.random.RandomState(1)\n",
    "X = 10 * rng.rand(100, 3)\n",
    "y = 0.5 + np.dot(X, [1.5, -2., 1.])\n",
    "model.fit(X, y)\n",
    "print(model.intercept_)\n",
    "print(model.coef_)\n",
    "Out[5]:\n",
    "0.5\n",
    "[ 1.5 -2. 1. ]\n",
    "\n",
    "In[6]:\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "x = np.array([2, 3, 4])\n",
    "poly = PolynomialFeatures(3, include_bias=False)\n",
    "poly.fit_transform(x[:, None])\n",
    "Out[6]:\n",
    "array([[ 2., 4., 8.], [ 3., 9., 27.], [ 4., 16., 64.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines - Klassifikationen und Regressionen\n",
    "zB Punkte in Klassen farblich unterteilen\n",
    "Jetzt geht es um diskriminative Klassifikationen: Anstatt die Klassen zu modellieren, versuchen wir,\n",
    "eine Gerade oder eine Kurve (in zwei Dimensionen) bzw. eine Mannigfaltigkeit (in mehreren\n",
    "Dimensionen) zu finden, mit der sich die Klassen voneinander trennen lassen.\n",
    "\n",
    "Ein linearer diskriminativer Klassifikator würde versuchen, eine Gerade zu finden, mit der sich die beiden\n",
    "Punktmengen voneinander trennen lassen, und somit ein Klassifikationsmodell erzeugen.\n",
    "Bei zweidimensionalen Aufgaben, wie der hier gezeigten, kann man dies von Hand erledigen.\n",
    "Problem: Es gibt mehr als eine Gerade, mit der sich die beiden Klassen perfekt voneinander trennen\n",
    "lassen.\n",
    "\n",
    "Support Vector Machines bieten eine Möglichkeit, diesen Sachverhalt zu verbessern, der folgende Idee\n",
    "zugrunde liegt: Anstatt eine einfache ausdehnungslose Gerade zwischen die Klassen zu legen, können\n",
    "wir entlang der Linie einen Randbereich bestimmter Breite zeichnen, den sogenannten Margin, der bis\n",
    "zum nächsten Punkt reicht.\n",
    "\n",
    "Anforderungen:\n",
    "Skalierung der Merkmale: SVMs sind sehr empfindlich gegenüber der Skalierung der Eingabedaten. Die Leistung des Modells kann erheblich beeinträchtigt werden, wenn die Merkmale nicht ordnungsgemäß skaliert sind, da SVMs versuchen, den größtmöglichen Margin zwischen den Klassen zu finden. Merkmale mit größeren Wertebereichen könnten unverhältnismäßig stark gewichtet werden.\n",
    "\n",
    "Auswahl der Kernel-Funktion: Die Wahl des Kernels ist entscheidend, da sie bestimmt, wie SVMs die Daten im höherdimensionalen Raum abbilden. Die Entscheidung, ob ein linearer, polynomialer, radialer Basisfunktions- (RBF) oder ein anderer Kernel verwendet wird, sollte auf der Natur der Daten und des Problems basieren.\n",
    "\n",
    "Hyperparameter-Tuning: SVMs haben kritische Hyperparameter wie den Regularisierungsparameter (C) und die Kernel-Parameter (z.B. Gamma im RBF-Kernel), die sorgfältig abgestimmt werden müssen, um eine optimale Leistung zu erzielen. Die richtige Einstellung dieser Parameter ist entscheidend für die Effektivität des Modells.\n",
    "\n",
    "Umgang mit unausgewogenen Datensätzen: SVMs können bei unausgewogenen Datensätzen zu Verzerrungen neigen. Die Anpassung der Klassen-Gewichte oder die Anwendung von Sampling-Methoden kann erforderlich sein, um eine faire Repräsentation beider Klassen im Modell zu gewährleisten.\n",
    "\n",
    "Empfindlichkeit gegenüber Ausreißern: SVMs können empfindlich auf Ausreißer reagieren, insbesondere weil sie die Entscheidungsgrenze auf der Grundlage der am nächsten liegenden Punkte (Support Vektoren) festlegen. Ausreißer können die Position der Entscheidungsgrenze erheblich beeinflussen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entscheidungsbäume und Random Forests - MÜSSEN IN ARBEIT!!\n",
    "Random Forests gehören zu den\n",
    "sogenannten Ensemblemethoden– Verfahren, die darauf beruhen, die Ergebnisse von einem\n",
    "Ensemble einfacherer Schätzer zusammenzutragen. Das etwas Überraschende an\n",
    "Ensemblemethoden ist, dass das Ergebnis mehr ist als die Summe der Teile. Beispielsweise kann die\n",
    "Mehrheitsentscheidung einer Reihe von Schätzern ein Resultat ergeben, das besser ist als alle\n",
    "Ergebnisse der einzelnen Schätzer.\n",
    "Entscheidungsbäume sind ein besonders intuitives Verfahren, Objekte zu klassifizieren: Sie stellen\n",
    "einfach eine Reihe von Fragen, deren Hauptaugenmerk auf die Klassifikation gerichtet sind.\n",
    "Der binäre Aufbau ist äußerst effizient: Bei einem gut durchdachten Baum wird die Anzahl der\n",
    "verbleibenden Optionen durch die Beantwortung jeder Frage in etwa halbiert, sodass selbst bei einer\n",
    "großen Anzahl von Klassen schnell nur noch einige wenige Möglichkeiten zur Auswahl stehen.\n",
    "\n",
    "Der Trick besteht natürlich darin, zu entscheiden, welche Fragen bei den einzelnen Schritten gestellt\n",
    "werden.\n",
    "Bei Entscheidungsbäumen, die für das Machine Learning konzipiert sind, werden die Fragen im\n",
    "Allgemeinen so formuliert, dass sie die Daten entlang einer der Koordinatenachsen aufteilen.\n",
    "Jeder Baumknoten unterteilt die Daten also anhand eines Schwellenwerts eines der Merkmale in zwei\n",
    "Gruppen.\n",
    "Overfitting: Entscheidungsbäume neigen dazu, sich an die Trainingsdaten zu überanpassen (Overfitting), besonders wenn der Baum sehr tief ist. Dies kann durch Techniken wie Baumbeschneidung oder Festlegung einer maximalen Tiefe des Baumes gemildert werden.\n",
    "\n",
    "\n",
    "Im letzten Abschnitt haben wir Random Forests im Kontext der Klassifikation betrachtet. Random\n",
    "Forests sind jedoch auch für Regressionen geeignet (die stetige statt kategoriale Werte liefern). Der\n",
    "hierfür geeignete Schätzer heißt RandomForestRegressor, und die Syntax ist der bisherigen sehr\n",
    "ähnlich.\n",
    "Ein Random Forest ist eine Ensemble-Methode, die mehrere Entscheidungsbäume während des Trainingsprozesses erstellt und deren Vorhersagen kombiniert.\n",
    "Reduzierung von Overfitting: Durch die Kombination der Vorhersagen von vielen Bäumen reduzieren Random Forests das Risiko des Overfittings, das bei einzelnen Entscheidungsbäumen auftritt.\n",
    "\n",
    "Anforderungen:\n",
    "Entscheidungsbäume:\n",
    "Merkmalsvorbereitung: Während Entscheidungsbäume sowohl numerische als auch kategoriale Daten verarbeiten können, müssen kategoriale Daten oft in eine geeignete numerische Form umgewandelt werden.\n",
    "Skalierung: Im Gegensatz zu vielen anderen Algorithmen benötigen Entscheidungsbäume keine Skalierung der Merkmale. Sie sind invariant gegenüber der Skalierung der Eingabedaten.\n",
    "Overfitting: Entscheidungsbäume neigen dazu, sich an die Trainingsdaten zu überanpassen, besonders wenn keine Beschränkungen für die Baumtiefe oder die Mindestanzahl von Stichproben pro Blatt festgelegt werden.\n",
    "\n",
    "Random Forests:\n",
    "Größe des Datensatzes: Random Forests benötigen in der Regel einen größeren Datensatz, um effektiv zu sein, da sie auf dem Prinzip des Ensemble-Lernens basieren, bei dem mehrere Bäume aus verschiedenen Teilen des Datensatzes erstellt werden.\n",
    "Diversität der Daten: Für eine optimale Leistung ist es wichtig, dass die Daten eine gewisse Diversität aufweisen. Random Forests profitieren von der Vielfalt in den Daten, da dies zu einer Vielzahl von Bäumen im Ensemble führt. Ausreißer und fehlende Daten: Random Forests sind im Allgemeinen robuster gegenüber Ausreißern und können besser mit fehlenden Daten umgehen als einzelne Entscheidungsbäume.\n",
    "Unausgewogene Datensätze: Random Forests können mit unausgewogenen Datensätzen umgehen, aber es kann notwendig sein, Techniken wie das Anpassen der Klassen-Gewichte oder Oversampling/Undersampling anzuwenden, um eine Verzerrung zu vermeiden.\n",
    "Rechenressourcen: Da Random Forests mehrere Entscheidungsbäume erstellen und kombinieren, können sie rechenintensiver sein, insbesondere bei großen Datensätzen oder einer großen Anzahl von Bäumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[2]:\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow');\n",
    "\n",
    "\n",
    "#Hilfsfunktion, die uns die Visualisierung der Ausgabe des Klassifikators\n",
    "In[5]:\n",
    "def visualize_classifier(model, X, y, ax=None, cmap='rainbow'):\n",
    "ax = ax or plt.gca()\n",
    "# Ausgabe der Trainingsdatenpunkte\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=cmap, clim=(y.min(), y.max()), zorder=3)\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "# Anpassen des Schätzers\n",
    "model.fit(X, y)\n",
    "xx, yy = np.meshgrid(np.linspace(*xlim, num=200), np.linspace(*ylim, num=200))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "# Farbiges Diagramm der Ergebnisse erstellen\n",
    "n_classes = len(np.unique(y))\n",
    "contours = ax.contourf(xx, yy, Z, alpha=0.3, levels=np.arange(n_classes + 1) - 0.5, cmap=cmap, clim=(y.min(), y.max()), zorder=1)\n",
    "ax.set(xlim=xlim, ylim=ylim)\n",
    "In[5]:\n",
    "visualize_classifier(DecisionTreeClassifier(), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hauptkomponentenanalyse (PCA) - unüberwacht - Dimensionsreduktionsalgorithmus\n",
    "Es werden keine Labels erkannt! \n",
    "Ist schnell und effektiv.\n",
    "sie kann auch für Visualisierung, Rauschunterdrückung, Merkmalsextraktion, Merkmalserstellung und vieles mehr verwendet werden.\n",
    "\n",
    "Dimensionalitätsreduktion: Der Hauptzweck der PCA ist die Reduzierung der Dimensionalität eines Datensatzes, während so viel der Variabilität (Information) wie möglich beibehalten wird. Dies wird erreicht, indem man neue, unkorrelierte Variablen (Hauptkomponenten) findet, die die größten Varianzen entlang ihrer Achsen aufweisen.\n",
    "\n",
    "Hauptkomponenten: Die Hauptkomponenten sind lineare Kombinationen der ursprünglichen Variablen. Die erste Hauptkomponente hat die größte Varianz, die zweite Hauptkomponente ist orthogonal zur ersten und hat die zweitgrößte Varianz, und so weiter.\n",
    "\n",
    "Varianzerhaltung: PCA zielt darauf ab, die Daten in eine geringere Anzahl von Dimensionen zu transformieren, wobei die Hauptkomponenten so gewählt werden, dass sie den größten Teil der Varianz in den ursprünglichen Daten erfassen.\n",
    "\n",
    "Anwendungen: PCA wird in vielen Bereichen eingesetzt, darunter Bildverarbeitung, Marktforschung, Genomik, Finanzanalyse und mehr, um Muster in Daten zu erkennen, Daten zu visualisieren, Rauschen zu reduzieren oder vorverarbeitete Daten für andere maschinelle Lernverfahren bereitzustellen.\n",
    "\n",
    "Interpretation: Die Interpretation der Hauptkomponenten kann nicht immer eindeutig sein, da sie Linearkombinationen der ursprünglichen Variablen sind. In einigen Fällen können sie jedoch sinnvolle Einblicke in die Struktur der Daten bieten.\n",
    "\n",
    "Limitationen: PCA ist effektiv bei der Identifizierung linearer Beziehungen, aber sie kann nichtlineare Beziehungen in den Daten übersehen. Außerdem kann die Reduzierung der Dimensionalität zu einem Informationsverlust führen, was bei der Entscheidung, wie viele Hauptkomponenten beibehalten werden sollen, berücksichtigt werden muss.\n",
    "\n",
    "Wenn hochdimensionale Daten vorliegen, führt man zunächst meist eine Hauptkomponentenanalyse zur\n",
    "Visualisierung der Beziehungen zwischen den Datenpunkten durch, um die entscheidende Varianz in\n",
    "den Daten zu erkennen und um die intrinsische Dimensionalität zu verstehen (durch Erstellen eines\n",
    "Diagramms der erklärten Varianz). Natürlich ist eine PCA nicht für alle hochdimensionalen Datenmengen\n",
    "geeignet, sie bietet jedoch eine unkomplizierte und effiziente Möglichkeit, Einblick in hochdimensionale\n",
    "Daten zu gewinnen.\n",
    "Der größte Schwachpunkt der Hauptkomponentenanalyse ist, dass sie sehr empfindlich auf Ausreißer in\n",
    "den Daten reagiert. Aus diesem Grund wurden viele sehr robuste PCA-Varianten entwickelt, die nach\n",
    "und nach Datenpunkte entfernen, die sich durch die anfänglichen Komponenten nur schlecht\n",
    "beschreiben lassen.\n",
    "Scikit-Learn enthält eine Reihe interessante PCA-Varianten wie RandomizedPCAund SparsePCA, die\n",
    "beide auch Teil des Submoduls sklearn.decompositionsind. Die bereits bekannte RandomizedPCA\n",
    "verwendet ein nicht deterministisches Verfahren zur schnellen Näherung der ersten paar\n",
    "Hauptkomponenten sehr hochdimensionaler Daten, SparsePCAhingegen führt einen\n",
    "Regularisierungsterm ein, der eine dünne Besetzung der Hauptkomponenten erzwingt.\n",
    "\n",
    "Anforderugen:\n",
    "Voraussetzungen: Für die effektive Anwendung der PCA sollten die Daten standardisiert werden, insbesondere wenn die Merkmale unterschiedliche Maßeinheiten oder sehr unterschiedliche Varianzen aufweisen.\n",
    "\n",
    "Skalierung der Merkmale: Die Merkmale sollten auf eine gemeinsame Skala gebracht werden, insbesondere wenn sie unterschiedliche Maßeinheiten oder Größenordnungen haben. Dies wird in der Regel durch Standardisierung erreicht, bei der jedes Merkmal so skaliert wird, dass es einen Mittelwert von 0 und eine Standardabweichung von 1 hat.\n",
    "\n",
    "Linearität: PCA setzt voraus, dass die Beziehungen zwischen den Merkmalen linear sind. Sie ist am effektivsten, wenn die Hauptkomponenten lineare Kombinationen der ursprünglichen Merkmale darstellen können.\n",
    "\n",
    "Große Varianz: PCA ist besonders nützlich, wenn es eine beträchtliche Varianz in den Daten gibt. Die Methode sucht nach Hauptkomponenten, die die größtmögliche Varianz erfassen. Wenn alle Variablen ähnlich sind oder nur eine geringe Varianz aufweisen, ist der Nutzen der PCA begrenzt.\n",
    "\n",
    "Große Stichprobengröße: Eine ausreichend große Stichprobengröße ist wünschenswert, um stabile Schätzungen der Kovarianz- bzw. Korrelationsmatrix zu gewährleisten. Eine Faustregel ist, dass die Stichprobengröße deutlich größer sein sollte als die Anzahl der Merkmale.\n",
    "\n",
    "Keine oder geringe Multikollinearität: Obwohl PCA oft verwendet wird, um Multikollinearität zu reduzieren, können extrem hohe Korrelationen zwischen Merkmalen die Interpretation der Hauptkomponenten erschweren.\n",
    "\n",
    "Ausreißer: Ausreißer können die Ergebnisse der PCA stark beeinflussen, da sie die Kovarianz- oder Korrelationsstruktur der Daten verzerren können. Es ist wichtig, die Daten auf Ausreißer zu überprüfen und diese gegebenenfalls zu behandeln.\n",
    "\n",
    "Normalverteilung der Merkmale: Obwohl dies keine strenge Voraussetzung ist, liefert PCA tendenziell bessere Ergebnisse, wenn die Merkmale annähernd normalverteilt sind, da die Kovarianz- und Korrelationsmatrizen in diesem Fall repräsentativer für die Daten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1]:\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "\n",
    "In[2]:\n",
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');\n",
    "\n",
    "In[3]:\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "Out[3]:\n",
    "PCA(copy=True, n_components=2, whiten=False)\n",
    "\n",
    "\n",
    "\n",
    "In[4]:\n",
    "print(pca.components_)\n",
    "Out[4]:\n",
    "[[ 0.94446029 0.32862557]\n",
    "[ 0.32862557 -0.94446029]]\n",
    "In[5]:\n",
    "print(pca.explained_variance_)\n",
    "Out[5]:\n",
    "[ 0.75871884 0.01838551]\n",
    "\n",
    "In[6]:\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "ax = ax or plt.gca()\n",
    "arrowprops=dict(arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0)\n",
    "ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "# Daten ausgeben\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "v = vector * 3 * np.sqrt(length)\n",
    "draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manifold Learning - Dimensionsreduktionsalgorithmus - für Daten in nichtlineare Beziehungen \n",
    "Die PCA ist zwar flexibel, schnell und leicht interpretierbar, funktioniert aber nicht besonders gut, wenn inden Daten nichtlineare Beziehungen vorhanden sind. Daher Manifold Learning!\n",
    "Manifold Learning, auch als nichtlineare Dimensionsreduktion bekannt, ist ein Ansatz im Bereich des maschinellen Lernens, der darauf abzielt, die Struktur und die Beziehungen in komplexen, hochdimensionalen Daten zu verstehen und zu vereinfachen. Im Gegensatz zu linearen Techniken wie der Hauptkomponentenanalyse (PCA), die auf linearen Transformationen basieren, kann Manifold Learning nichtlineare Strukturen in den Daten erfassen.\n",
    "\n",
    "Anforderungen:\n",
    "Repräsentativität der Daten: Die Daten sollten die zugrundeliegende Struktur des Problems gut repräsentieren. Manifold Learning zielt darauf ab, die inhärente Struktur in den Daten zu entdecken, was nur möglich ist, wenn die Daten diese Struktur auch tatsächlich widerspiegeln.\n",
    "\n",
    "Angemessene Stichprobengröße: Eine ausreichende Menge an Daten ist erforderlich, um die Struktur des Manifolds zuverlässig zu erfassen. Zu wenige Daten können zu einer unzureichenden oder verzerrten Darstellung des Manifolds führen.\n",
    "\n",
    "Skalierung der Merkmale: Obwohl dies nicht immer erforderlich ist, kann die Skalierung oder Normalisierung der Merkmale dazu beitragen, dass alle Merkmale gleichmäßig zur Analyse beitragen, insbesondere wenn sie unterschiedliche Maßeinheiten oder Größenordnungen haben.\n",
    "\n",
    "Nichtlineare Beziehungen: Manifold Learning ist besonders nützlich, wenn die Daten nichtlineare Beziehungen enthalten, die mit linearen Methoden wie der PCA nicht erfasst werden können.\n",
    "\n",
    "Dimensionalität der Daten: Manifold Learning ist besonders geeignet für Daten mit hoher Dimensionalität, bei denen die Annahme gilt, dass die Daten auf einem niedrigerdimensionalen Manifold liegen.\n",
    "\n",
    "Keine oder geringe Multikollinearität: Obwohl Manifold Learning dazu beitragen kann, Multikollinearität zu reduzieren, können extrem hohe Korrelationen zwischen Merkmalen die Fähigkeit des Verfahrens, die wahre Struktur in den Daten zu entdecken, beeinträchtigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1]:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "In[3]:\n",
    "X = make_hello(1000)\n",
    "colorize = dict(c=X[:, 0], cmap=plt.cm.get_cmap('rainbow', 5))\n",
    "plt.scatter(X[:, 0], X[:, 1], **colorize)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Means-Clustering - Clustering-Algorithmus\n",
    "Clustering-Algorithmen versuchen, anhand der Eigenschaften der\n",
    "Daten eine optimale Aufteilung der Datenpunkte in unterschiedliche Gruppen vorzunehmen (Zuordnung zu diskreten Labels).\n",
    "\n",
    "Der k-Means-Algorithmus sucht in einer ungekennzeichneten multidimensionalen Datenmenge nach\n",
    "einer vorgegebenen Anzahl von Clustern. Dabei kommt ein einfacher Satz von Regeln zum Einsatz, die\n",
    "beschreiben, wie eine optimale Aufteilung aussieht:\n",
    "• Das Zentrum eines Clusters ist das arithmetische Mittel aller zum Cluster zugehörigen\n",
    "Datenpunkte.\n",
    "• Der Abstand eines Punkts zum Zentrum des eigenen Clusters ist geringer als der Abstand zu den\n",
    "Zentren anderer Cluster.\n",
    "Diese beiden Annahmen sind die Grundlage des k-Means-Modells.\n",
    "Zunächst erstellen wir eine zweidimensionale Datenmenge, die vier verschiedene Anhäufungen von\n",
    "Punkten enthält. Um hervorzuheben, dass es sich um einen unüberwachten Algorithmus handelt, lassen\n",
    "wir in der Visualisierung die Labels weg (siehe nächste Abbildung).\n",
    "\n",
    "Das Schöne daran ist, dass der k-Means-Algorithmus die Zuweisung der Punkte zu den Clustern ganz ähnlich\n",
    "vornimmt, wie wir es mit bloßem Auge tun (zumindest in diesem einfachen Fall). Aber Sie werden sich vielleicht\n",
    "fragen, wie der Algorithmus die Cluster so schnell findet. Schließlich wächst die Zahl der Kombinationen möglicher\n",
    "Cluster-Zuweisungen exponentiell mit der Anzahl der Datenpunkte – eine vollständige Suche wäre sehr, sehr\n",
    "rechenaufwendig. Erfreulicherweise ist eine solche vollständige Suche aber gar nicht notwendig. Stattdessen\n",
    "verwendet das k-Means-Verfahren einen iterativen Ansatz, der als Expectation-Maximization-Algorithmus (kurz\n",
    "EM-Algorithmus) bezeichnet wird.\n",
    "\n",
    "Expectation-Maximization (EM) ist ein leistungsfähiger Algorithmus, der in der Data Science in verschiedenen\n",
    "Kontexten auftaucht. k-Means ist eine besonders einfache und gut verständliche Anwendung dieses Algorithmus, den\n",
    "wir nun kurz betrachten. Kurz und bündig gesagt, wird beim EM-Ansatz folgendermaßen verfahren:\n",
    "1. Zufällige Auswahl einiger Cluster-Zentren.\n",
    "2. Wiederholung der nächsten beiden Schritte, bis das Verfahren konvergiert.\n",
    "a) Expectation-Schritt: Den Punkten wird das nächstgelegene Cluster-Zentrum zugeordnet.\n",
    "b) Maximization-Schritt: Den Cluster-Zentren wird der Mittelwert zugewiesen.\n",
    "Der Expectation-Schritt (E-Schritt) heißt so, weil wir die Erwartung, zu welchem Cluster die verschiedenen Punkte\n",
    "gehören, bei diesem Schritt aktualisieren. Und der Maximization-Schritt (M-Schritt) trägt seinen Namen, weil wir eine\n",
    "Gütefunktion maximieren, die den Ort der Cluster-Zentren festlegt. In diesem Fall wird die Maximierung dadurch\n",
    "erzielt, dass wir einen einfachen Mittelwert der Daten in den Clustern berechnen. Typischerweise führt jede\n",
    "Wiederholung des Expectation- und Maximization-Schritt zu einer verbesserten Abschätzung der Cluster-\n",
    "Eigenschaften.\n",
    "\n",
    "Probleme: \n",
    "Schlechte Konvergenz: Zwar ist sichergestellt, dass der EM-Algorithmus das Ergebnis bei jedem Schritt verbessert, es ist aber nicht\n",
    "garantiert, dass er die beste globale Lösung findet.\n",
    "\n",
    "Anzahl der Cluster vorher festlegen: Dass man im Vorhinein die Anzahl der Cluster festlegen muss, ist beim k-Means-Algorithmus mit EM-Verfahren\n",
    "ebenfalls problematisch – er kann die Anzahl nicht anhand der Daten ermitteln.\n",
    "\n",
    "Lineare Clustergrenzen: Die grundlegenden Annahmen des k-Means-Modells (Punkte sind ihrem eigenen Cluster- Zentrum näher als anderen\n",
    "Cluster-Zentren) führen oft dazu, dass der Algorithmus nur schlecht funktioniert, wenn die Cluster komplizierte\n",
    "Formen besitzen.\n",
    "Die Grenzen zwischen k-Means-Clustern sind immer linear, daher funktioniert das Verfahren bei komplizierteren\n",
    "Grenzen nicht.\n",
    "\n",
    "Langsam bei großer Anzahl von Mustern: Da der Algorithmus bei jeder Iteration auf sämtliche Punkte der Datenmenge zugreifen muss, ist er relativ langsam,\n",
    "wenn die Anzahl der Datenpunkte groß ist.\n",
    "\n",
    "Anforderungen:\n",
    "Die Anzahl der Cluster (k) muss im Voraus festgelegt werden.\n",
    "Der Algorithmus funktioniert am besten bei Clustern, die in etwa gleich groß und kugelförmig sind.\n",
    "Er ist empfindlich gegenüber Ausreißern und Rauschen, da diese die Berechnung der Clusterzentren stark beeinflussen können.\n",
    "\n",
    "Skalierung der Merkmale: Die Merkmale sollten auf eine gemeinsame Skala gebracht werden, da k-Means auf der euklidischen Distanz basiert. Unterschiedliche Maßstäbe der Merkmale können zu Verzerrungen führen, da Merkmale mit größeren Wertebereichen einen unverhältnismäßig großen Einfluss auf die Clusterbildung haben können.\n",
    "\n",
    "Homogene Varianz: k-Means geht davon aus, dass alle Cluster eine ähnliche Varianz haben. Wenn einige Cluster deutlich dichter sind als andere, kann dies die Leistung des Algorithmus beeinträchtigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1]:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set() # für Diagrammstile\n",
    "import numpy as np\n",
    "\n",
    "In[2]:\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=50);\n",
    "\n",
    "In[3]:\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)\n",
    "\n",
    "In[4]:\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis')\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "\n",
    "In[5]:\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "# 1. Zufällige Auswahl der Cluster\n",
    "rng = np.random.RandomState(rseed)\n",
    "i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "centers = X[i]\n",
    "while True:\n",
    "# 2a. Labels anhand der nächstgelegenen\n",
    "# Cluster-Zentren zuweisen\n",
    "labels = pairwise_distances_argmin(X, centers)\n",
    "# 2b. Neue Zentren aus Mittelwert errechnen\n",
    "new_centers = np.array([X[labels == i].mean(0) for i in range(n_clusters)])\n",
    "# 2c. Konvergenz prüfen\n",
    "if np.all(centers == new_centers):\n",
    "break\n",
    "centers = new_centers\n",
    "return centers, labels\n",
    "centers, labels = find_clusters(X, 4)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauß’sche Mixture-Modelle - \"Clustering-Algorithmus\" - Erweiterung der dem k-Means-Algorithmus\n",
    "\n",
    "Das im letzten Abschnitt vorgestellte k-Means-Modell ist einfach und relativ gut verständlich, aber eben diese\n",
    "Einfachheit kann bei der Anwendung zu Problemen führen. Insbesondere die nicht probabilistische Natur des k-\n",
    "Means-Verfahrens und die Verwendung eines simplen Abstands vom Cluster-Zentrum führen in der Praxis in vielen\n",
    "Fällen zu schlechten Ergebnissen. In diesem Abschnitt betrachten wir das Gauß’sche Mixture-Modell (GMM), das\n",
    "man als Erweiterung der dem k-Means-Algorithmus zugrunde liegenden Idee auffassen kann, das aber auch als\n",
    "leistungsfähiges Werkzeug für Abschätzungen jenseits des einfachen Clusterings dient.\n",
    "\n",
    "GMM wird zwar oft als Clustering-Algorithmus eingestuft, ist aber grundsätzlich ein Algorithmus zur\n",
    "Dichteschätzung. Das soll heißen, dass das Ergebnis eines GMM-Fits an irgendwelche Daten technisch gesehen\n",
    "kein Clustering-Modell, sondern ein generatives probabilistisches Modell ist, das die Verteilung der Daten\n",
    "beschreibt.\n",
    "\n",
    "Komponenten: Ein GMM setzt sich aus mehreren Gaußschen (normalverteilten) Komponenten zusammen. Jede Komponente repräsentiert eine Subpopulation und wird durch ihre eigene Mittelwert (Mittelwertvektor im mehrdimensionalen Raum) und Kovarianz charakterisiert.\n",
    "\n",
    "Wahrscheinlichkeitsbasierte Zugehörigkeit: Im Gegensatz zu k-Means, das jedem Datenpunkt genau einem Cluster zuordnet, weist GMM jedem Datenpunkt eine Wahrscheinlichkeit zu, zu jeder der Gaußschen Komponenten zu gehören. Diese Wahrscheinlichkeiten geben an, wie wahrscheinlich es ist, dass ein Datenpunkt von einer bestimmten Komponente generiert wurde.\n",
    "\n",
    "Flexibilität: GMMs sind flexibler als k-Means, da sie nicht nur die Zentren der Cluster, sondern auch deren Form und Größe (über die Kovarianz der Komponenten) modellieren. Dies ermöglicht es GMMs, mit elliptischen und überlappenden Clustern umzugehen.\n",
    "\n",
    "Hinter den Kulissen ist ein GMM einem k-Means-Modell sehr ähnlich. Es verwendet einen Expectation-\n",
    "Maximization-Ansatz und funktioniert qualitativ folgendermaßen:\n",
    "1. Auswahl der Anfangsbedingungen für Ort und Form.\n",
    "2. Wiederholung der nächsten beiden Schritte, bis das Verfahren konvergiert.\n",
    "a) Expectation-Schritt: Gewichtungen berechnen, die ein Maß für die Wahrscheinlichkeit sind, dass ein Punkt zu einem Cluster\n",
    "gehört.\n",
    "b) Maximization-Schritt: Ort, Normalisierung und Form der Cluster unter Berücksichtigung aller Datenpunkte aktualisieren; dabei\n",
    "wird von den Gewichtungen Gebrauch gemacht.\n",
    "Auf diese Weise wird jedem Cluster eine Sphäre zugeordnet, die als eindeutige harte Grenze dient, aber auf einem\n",
    "geglätteten Gaußmodell beruht. Wie der k-Means-Ansatz mit EM übersieht auch dieser Algorithmus manchmal die\n",
    "globale optimale Lösung, daher werden in der Praxis mehrere zufällig ausgewählte Anfangsbedingungen\n",
    "verwendet.\n",
    "\n",
    "Wie viele Komponenten?\n",
    "Die Tatsache, dass das GMM ein generatives Modell ist, ermöglicht uns, die optimale Anzahl der Komponenten\n",
    "für eine gegebene Datenmenge zu berechnen. Ein generatives Modell stellt eine Wahrscheinlichkeitsverteilung für\n",
    "die Datenmenge dar, wir können es also dazu nutzen, die Wahrscheinlichkeiten der Daten zu ermitteln, und dabei\n",
    "eine Kreuzvalidierung einsetzen, um eine Überanpassung zu verhindern.\n",
    "Eine weitere Möglichkeit, einer Überanpassung Rechnung zu tragen, ist eine Korrektur der\n",
    "Modellwahrscheinlichkeiten anhand analytischer Kennwerte, etwa des Akaike- (AIC) oder des bayesschen\n",
    "Informationskriteriums (BIC). Tatsächlich sind in Scikit-Learns GMM-Schätzer bereits Methoden integriert, die diese\n",
    "Werte berechnen, daher ist es ganz einfach, den Ansatz zu verfolgen.\n",
    "\n",
    "\n",
    "Anfordeungen:\n",
    "Normalverteilte Subpopulationen: GMMs gehen davon aus, dass die Daten aus einer Mischung von mehreren normalverteilten Subpopulationen bestehen. Die Methode ist am effektivsten, wenn diese Annahme zumindest annähernd zutrifft.\n",
    "\n",
    "Bestimmung der Komponentenanzahl: Die Anzahl der Gaußschen Komponenten im Modell muss im Voraus festgelegt werden. Methoden wie das Bayesianische Informationskriterium (BIC) können dabei helfen, aber eine gewisse Kenntnis oder Annahme über die Struktur der Daten ist nützlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1]:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kerndichteschätzung - Dichteschätzer - Alternative zu Histogrammen\n",
    "\n",
    "Wie Sie wissen, ist ein Dichteschätzer ein Algorithmus, der eine D-dimensionale Datenmenge entgegennimmt und\n",
    "eine Abschätzung der D-dimensionalen Wahrscheinlichkeitsverteilung der Daten liefert, aus der die Datenmenge\n",
    "stammt. Der GMM-Algorithmus erreicht das durch die Repräsentierung der Dichte als gewichtete Summe von\n",
    "Gaußverteilungen.\n",
    "Die Kerndichteschätzung (engl. Kernel Density Estimation, kurz KDE) ist ein Algorithmus, der die dem GMM\n",
    "zugrunde liegende Idee gewissermaßen konsequent zu Ende führt: Er verwendet eine Mischung, die für jeden\n",
    "Datenpunkt eine Gaußkomponente enthält, was einen im Wesentlichen unparametrisierten Dichteschätzer ergibt.\n",
    "\n",
    "Im Fall eindimensionaler Daten sind Sie vermutlich schon mit einem\n",
    "einfachen Dichteschätzer vertraut: dem Histogramm. Ein Histogramm unterteilt die Daten in diskrete\n",
    "Wertebereiche (sogenannte Bins), zählt die Anzahl der Punkte, die zu einem Wertebereich gehören, und visualisiert\n",
    "das Ergebnis auf verständliche Weise.\n",
    "\n",
    "Das Problem bei der Aufteilung in Bins gleicher Breite beruht auf der Tatsache, dass die Höhe des Bausteinstapels\n",
    "oft nicht die tatsächliche Dichte nahe gelegener Punkte widerspiegelt, sondern auch von der zufälligen Lage von\n",
    "Punkten im Verhältnis zu den Bins abhängt. Diese fehlende Übereinstimmung von Punkten und Bausteinen ist eine\n",
    "mögliche Ursache für das schlechte Ergebnis der Histogramme.\n",
    "\n",
    "Die geglättete Kurve mit den Beiträgen einer Gaußverteilung an den Orten der Eingabepunkte vermittelt eine sehr viel\n",
    "genauere Vorstellung von der Form der Datenverteilung, die darüber hinaus eine deutlich geringere Varianz besitzt\n",
    "(also weniger empfindlich auf Änderungen in der Stichprobe reagiert).\n",
    "\n",
    "Scikit-Learnn ist dabei hilfreich.\n",
    "\n",
    "Nichtparametrisch: Im Gegensatz zu parametrischen Methoden, die eine spezifische Verteilungsform annehmen (wie Normalverteilung), macht KDE keine Annahmen über die Form der Verteilung. Dies macht sie flexibel und nützlich für explorative Datenanalyse.\n",
    "\n",
    "Kernfunktionen: KDE funktioniert, indem sie um jeden Datenpunkt eine Kernfunktion (oder Fensterfunktion) platziert und dann alle diese Funktionen überlagert, um eine glatte Schätzung der Dichtefunktion zu erhalten. Häufig verwendete Kernfunktionen sind der Gaußsche Kern (Normalverteilung) und der Epanechnikov-Kern.\n",
    "\n",
    "Bandbreite: Ein kritischer Parameter bei der KDE ist die Bandbreite (oder Fensterbreite). Die Bandbreite steuert, wie glatt die geschätzte Dichtefunktion ist. Eine zu kleine Bandbreite führt zu einer übermäßig zackigen Schätzung (Overfitting), während eine zu große Bandbreite die Daten zu sehr glättet und Details verlieren kann (Underfitting).\n",
    "\n",
    "Vergleich mit Histogrammen: KDE kann als eine fortgeschrittene Alternative zu Histogrammen angesehen werden. Während Histogramme die Daten in diskrete Bins einteilen, liefert KDE eine kontinuierliche Dichteschätzung, die oft eine intuitivere Darstellung der Datenverteilung bietet.\n",
    "\n",
    "Bei der Kerndichteschätzung ist die Auswahl der Bandbreite von entscheidender Bedeutung. Sie ist die\n",
    "Stellschraube, die bei der Abschätzung der Dichte den Kompromiss zwischen Bias und Varianz regelt: Eine zu\n",
    "schmale Bandbreite führt zu einer Abschätzung mit hoher Varianz (es kommt zu einer Überanpassung), bei der\n",
    "das Vorhandensein oder die Abwesenheit nur eines einzelnen Punkts schon einen großen Unterschied bedeutet.\n",
    "Eine zu weite Bandbreite hingegen führt zu einer Abschätzung mit großem Bias (es kommt zu einer\n",
    "Unteranpassung), wobei die Struktur in den Daten durch den weiten Kern verwässert wird.\n",
    "\n",
    "Bandbreite ermitteln!\n",
    "\n",
    "Probleme:\n",
    "Die Position der Bins beeinflusst die Interpretation\n",
    "\n",
    "Anforderungen:\n",
    "Multivariate Daten: Bei der Anwendung von KDE auf multivariate Daten ist es wichtig, dass die Dimensionalität der Daten nicht zu hoch ist, da KDE in höheren Dimensionen an Effektivität verlieren kann (Fluch der Dimensionalität).\n",
    "\n",
    "Bandbreitenwahl: Die Wahl der Bandbreite ist entscheidend für die Qualität der Dichteschätzung. Eine zu kleine Bandbreite kann zu einer übermäßig zackigen Schätzung führen, während eine zu große Bandbreite wichtige Details der Datenstruktur glätten kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In[1]:\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "\n",
    "In[2]:\n",
    "def make_data(N, f=0.3, rseed=1):\n",
    "rand = np.random.RandomState(rseed)\n",
    "x = rand.randn(N)\n",
    "x[int(f * N):] += 5\n",
    "return x\n",
    "x = make_data(1000)\n",
    "\n",
    "In[9]:\n",
    "from scipy.stats import norm\n",
    "x_d = np.linspace(-4, 8, 1000)\n",
    "density = sum(norm(xi).pdf(x_d) for xi in x)\n",
    "plt.fill_between(x_d, density, alpha=0.5)\n",
    "plt.plot(x, np.full_like(x, -0.1), '|k', markeredgewidth=1)\n",
    "plt.axis([-4, 8, -0.2, 5]);"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
